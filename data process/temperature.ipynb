{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading files: 100%|██████████| 732/732 [00:54<00:00, 13.45file/s]\n"
     ]
    }
   ],
   "source": [
    "#โฟลเดอร์\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def read_files_in_fraction(directory_path, fraction_start, fraction_end):\n",
    "    # Get a list of file names in the directory\n",
    "    file_names = os.listdir(directory_path)\n",
    "\n",
    "    # Calculate the number of files to read based on the fraction\n",
    "    num_files_to_read_start = int(len(file_names) * fraction_start)\n",
    "    num_files_to_read_end = int(len(file_names) * fraction_end)\n",
    "\n",
    "    # Initialize an empty list to store individual DataFrames\n",
    "    dataframes = []\n",
    "\n",
    "    # Use tqdm to create a progress bar for reading files\n",
    "    for file_name in tqdm(file_names[num_files_to_read_start:num_files_to_read_end], desc=\"Reading files\", unit=\"file\"):\n",
    "        if file_name.endswith('.csv'):  # Adjust the file extension based on your files (e.g., .csv, .xlsx, etc.)\n",
    "            file_path = os.path.join(directory_path, file_name)\n",
    "            df2 = pd.read_csv(file_path)  # Replace 'read_csv' with 'read_excel' for Excel files\n",
    "            dataframes.append(df2)\n",
    "\n",
    "    # Concatenate all the individual DataFrames into a single DataFrame\n",
    "    df3 = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "    return df3\n",
    "\n",
    "# Replace 'directory_path' with the path to the directory containing your files\n",
    "directory_path = 'E:\\\\CUSC Datalake\\\\cham4'  #<<<<<<<<<<<<<\n",
    "\n",
    "# Read 1/4 of the files\n",
    "df = read_files_in_fraction(directory_path, 0.75, 1)  #<<<<<<<<<<<<<\n",
    "\n",
    "file_save_name = r\"E:\\Sort Data\\cham4_temp\\cham4_temperature_4.csv\"  #<<<<<<<<<<<<<\n",
    "\n",
    "# Filter rows where \"id\" column contains \"multi_sensor\"\n",
    "df = df[df['id'].str.contains('temperature')]\n",
    "\n",
    "# Convert 'value' column to numeric (float) type if it's not already done\n",
    "df['value'] = pd.to_numeric(df['value'], errors='coerce')\n",
    "\n",
    "# Drop rows with value 0 in the \"value\" column\n",
    "df = df[df['value'] != 0]\n",
    "\n",
    "#extract id_floor\n",
    "df['id_floor'] = df['id'].str.extract(r'(fl(?:m|\\d{1,2}[a-zA-Z]?))')\n",
    "\n",
    "#sort\n",
    "df = df.sort_values(by=['id_floor','time']) \n",
    "\n",
    "# Remove the 'id' column\n",
    "df = df.drop('id', axis=1)\n",
    "\n",
    "# Convert 'time' column to pandas datetime object\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "# Round the minute to the nearest minute\n",
    "df['time'] = df['time'].dt.round('1min')\n",
    "\n",
    "# Export the result to a CSV file\n",
    "df.to_csv(file_save_name, index=False)\n",
    "\n",
    "df = pd.read_csv(file_save_name)\n",
    "\n",
    "# Group by 'time' and calculate the average temperature for each unique timestamp\n",
    "df['value'] = df.groupby(['id_floor', 'time'])['value'].transform('mean')\n",
    "\n",
    "# Format the 'value' column to two decimal places\n",
    "df['value'] = df['value'].round(2)\n",
    "\n",
    "# Drop rows with the same time\n",
    "df = df.drop_duplicates(subset=['time', 'id_floor'], keep='first')\n",
    "\n",
    "# rename\n",
    "df.rename(columns={'value': 'temperature'}, inplace=True)\n",
    "\n",
    "df.to_csv(file_save_name, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#หาไอดีไม่ซ้ำ\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.read_csv(r\"E:\\CUSC Datalake\\cham4\\cham4-2022-09-01T0800.csv\")\n",
    "\n",
    "unique_id_column = df.drop_duplicates(subset='id')['id']\n",
    "\n",
    "unique_id_column.to_csv('E:\\\\Sort Data\\\\cham4_id.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id_floor                      time  energy  temperature\n",
      "0           fl1 2022-09-10 09:00:00+08:00   2.525        28.90\n",
      "12          fl1 2022-09-10 09:01:00+08:00   2.592        28.90\n",
      "15          fl1 2022-09-10 09:02:00+08:00   2.553        28.90\n",
      "22          fl1 2022-09-10 09:03:00+08:00   4.292        28.90\n",
      "30          fl1 2022-09-10 09:04:00+08:00  12.456        28.90\n",
      "...         ...                       ...     ...          ...\n",
      "282284      fl7 2022-12-31 23:55:00+08:00  60.444        29.41\n",
      "282292      fl7 2022-12-31 23:56:00+08:00  60.462        29.89\n",
      "282302      fl7 2022-12-31 23:57:00+08:00  60.355        29.89\n",
      "282308      fl7 2022-12-31 23:58:00+08:00  55.183        29.51\n",
      "282316      fl7 2022-12-31 23:59:00+08:00  53.679        29.28\n",
      "\n",
      "[282317 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# รวม energy & temperature\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "# def read_files_in_fraction(directory_path, fraction_start, fraction_end):\n",
    "#     # Get a list of file names in the directory\n",
    "#     file_names = os.listdir(directory_path)\n",
    "\n",
    "#     # Calculate the number of files to read based on the fraction\n",
    "#     num_files_to_read_start = int(len(file_names) * fraction_start)\n",
    "#     num_files_to_read_end = int(len(file_names) * fraction_end)\n",
    "\n",
    "#     # Initialize an empty list to store individual DataFrames\n",
    "#     dataframes = []\n",
    "\n",
    "#     # Use tqdm to create a progress bar for reading files\n",
    "#     for file_name in tqdm(file_names[num_files_to_read_start:num_files_to_read_end], desc=\"Reading files\", unit=\"file\"):\n",
    "#         if file_name.endswith('.csv'):  # Adjust the file extension based on your files (e.g., .csv, .xlsx, etc.)\n",
    "#             file_path = os.path.join(directory_path, file_name)\n",
    "#             df2 = pd.read_csv(file_path)  # Replace 'read_csv' with 'read_excel' for Excel files\n",
    "#             dataframes.append(df2)\n",
    "\n",
    "#     # Concatenate all the individual DataFrames into a single DataFrame\n",
    "#     df3 = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "#     return df3\n",
    "\n",
    "# # Replace 'directory_path' with the path to the directory containing your files\n",
    "# directory_path = 'E:\\\\CUSC Datalake\\\\cham4'  #<<<<<<<<<<<<<\n",
    "\n",
    "# # Read 1/4 of the files\n",
    "# df1 = read_files_in_fraction(directory_path, 0.25, 0.5)  #<<<<<<<<<<<<<\n",
    "\n",
    "#read files\n",
    "df1 = pd.read_csv(r\"E:\\Sort Data\\cham4_temp\\cham4_temperature_4.csv\")  #<<<<<<<<<<<<<<\n",
    "df2 = pd.read_csv(r\"E:\\Sort Data\\cham4_temp\\cham4_temperature_4.csv\")  #<<<<<<<<<<<<<<\n",
    "\n",
    "# Convert 'time' columns to datetime objects for both DataFrames\n",
    "df1['time'] = pd.to_datetime(df1['time'])\n",
    "df2['time'] = pd.to_datetime(df2['time'])\n",
    "\n",
    "# Sort both DataFrames by 'time'\n",
    "df1 = df1.sort_values(by='time')\n",
    "df2 = df2.sort_values(by='time')\n",
    "\n",
    "#rename columns\n",
    "df1.rename(columns={'value': 'energy'}, inplace=True)\n",
    "df1.rename(columns={'id_group': 'id_floor'}, inplace=True)\n",
    "\n",
    "# Merge the two DataFrames based on 'time' and 'id_floor' using merge_asof\n",
    "result = pd.merge_asof(df1, df2, on='time', by='id_floor', direction='forward', tolerance=pd.Timedelta('5 minutes'))\n",
    "\n",
    "#sort\n",
    "result = result.sort_values(by=['id_floor','time'])\n",
    "\n",
    "print(result)\n",
    "\n",
    "result.to_csv(r\"E:\\Sort Data\\cham4_energy&temp_4.csv\", index=False)  #<<<<<<<<<<<<<<"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "a=r\"E:\\Sort Data\\cham4_mix\\cham4_4variable_4.csv\"\n",
    "\n",
    "df = pd.read_csv(a)\n",
    "\n",
    "#df = df.sort_values(by=['id_floor','time'])\n",
    "\n",
    "df.rename(columns={'value': 'energy'}, inplace=True)\n",
    "#df.rename(columns={'id_group': 'id_floor'}, inplace=True)\n",
    "\n",
    "#Count the number of empty energy and temperature values\n",
    "#num_empty_energy = df['energy'].isnull().sum()\n",
    "#num_empty_temperature = df['temperature'].isnull().sum()\n",
    "\n",
    "#print(f\"Number of empty energy values: {num_empty_energy}\")\n",
    "#print(f\"Number of empty temperature values: {num_empty_temperature}\")\n",
    "\n",
    "df.to_csv(a, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique 'time' values in df1:\n",
      "<DatetimeArray>\n",
      "['2022-09-10 09:00:00+08:00', '2022-09-10 09:01:00+08:00',\n",
      " '2022-09-10 09:02:00+08:00', '2022-09-10 09:03:00+08:00',\n",
      " '2022-09-10 09:04:00+08:00', '2022-09-10 09:05:00+08:00',\n",
      " '2022-09-10 09:06:00+08:00', '2022-09-10 09:07:00+08:00',\n",
      " '2022-09-10 09:08:00+08:00', '2022-09-10 09:09:00+08:00',\n",
      " ...\n",
      " '2022-12-18 22:46:00+08:00', '2022-12-19 03:01:00+08:00',\n",
      " '2022-12-19 08:16:00+08:00', '2022-12-19 10:01:00+08:00',\n",
      " '2022-12-19 11:16:00+08:00', '2022-12-19 13:41:00+08:00',\n",
      " '2022-12-19 13:46:00+08:00', '2022-12-20 11:16:00+08:00',\n",
      " '2022-12-22 12:16:00+08:00', '2022-12-29 15:13:00+08:00']\n",
      "Length: 40345, dtype: datetime64[ns, UTC+08:00]\n",
      "\n",
      "Unique 'time' values in df2:\n",
      "<DatetimeArray>\n",
      "['2022-09-10 09:03:00+08:00', '2022-09-10 09:08:00+08:00',\n",
      " '2022-09-10 09:13:00+08:00', '2022-09-10 09:17:00+08:00',\n",
      " '2022-09-10 09:18:00+08:00', '2022-09-10 09:23:00+08:00',\n",
      " '2022-09-10 09:28:00+08:00', '2022-09-10 09:33:00+08:00',\n",
      " '2022-09-10 09:38:00+08:00', '2022-09-10 09:43:00+08:00',\n",
      " ...\n",
      " '2022-12-31 01:27:00+08:00', '2022-12-31 01:32:00+08:00',\n",
      " '2022-12-31 01:42:00+08:00', '2022-12-31 01:47:00+08:00',\n",
      " '2022-12-31 02:02:00+08:00', '2022-12-31 02:17:00+08:00',\n",
      " '2022-12-31 02:42:00+08:00', '2022-12-31 02:47:00+08:00',\n",
      " '2022-12-31 02:57:00+08:00', '2022-12-31 03:12:00+08:00']\n",
      "Length: 39853, dtype: datetime64[ns, UTC+08:00]\n",
      "\n",
      "Unique 'id_floor' values in df1:\n",
      "['fl1' 'fl2' 'fl3' 'fl4' 'fl5' 'fl6' 'fl7']\n",
      "\n",
      "Unique 'id_floor' values in df2:\n",
      "['fl1' 'fl2' 'fl3' 'fl4' 'fl5' 'fl6' 'fl7']\n",
      "Empty DataFrame\n",
      "Columns: [id_floor, time, temperature, humidity, intensity_x, intensity_y]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp/ipykernel_13500/2701108392.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  different_times_and_floor_df.drop(['energy'], axis=1, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "df1 = pd.read_csv(r\"E:\\Sort Data\\cham4_mix\\cham4_4variable_4.csv\")\n",
    "df2 = pd.read_csv(r\"E:\\Sort Data\\cham4_intensity\\cham4_intensity_4.csv\")\n",
    "\n",
    "# Convert the 'time' column to datetime type\n",
    "df1['time'] = pd.to_datetime(df1['time'])\n",
    "df2['time'] = pd.to_datetime(df2['time'])\n",
    "\n",
    "# Print unique values in 'time' and 'id_floor' columns in both DataFrames\n",
    "print(\"Unique 'time' values in df1:\")\n",
    "print(df1['time'].unique())\n",
    "print(\"\\nUnique 'time' values in df2:\")\n",
    "print(df2['time'].unique())\n",
    "\n",
    "print(\"\\nUnique 'id_floor' values in df1:\")\n",
    "print(df1['id_floor'].unique())\n",
    "print(\"\\nUnique 'id_floor' values in df2:\")\n",
    "print(df2['id_floor'].unique())\n",
    "\n",
    "# Perform an SQL-style merge (join) on 'time' and 'id_floor' columns\n",
    "merged_df = pd.merge(df1, df2, on=['time', 'id_floor'], how='outer')\n",
    "\n",
    "# Filter rows where the 'time' and 'id_floor' are not the same in both dataframes\n",
    "different_times_and_floor_df = merged_df[~(merged_df['time'] == merged_df['time'])]\n",
    "\n",
    "# Drop the additional columns used for the merge (optional)\n",
    "different_times_and_floor_df.drop(['energy'], axis=1, inplace=True)\n",
    "\n",
    "# Display the result\n",
    "print(different_times_and_floor_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
